{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/coldsober-irene/hackathon1/blob/main/Irene_Nsengumukiza_Hackathon_File.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installation zone**"
      ],
      "metadata": {
        "id": "TS7wXpo9JbDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install  -U easynmt"
      ],
      "metadata": {
        "id": "8xRwH7UXJgxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**All required imports**"
      ],
      "metadata": {
        "id": "bf4vZkuWJPs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup as bs\n",
        "import requests\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "from easynmt import EasyNMT\n",
        "import json # to access languages codes"
      ],
      "metadata": {
        "id": "SDwJu4TMJOtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||  **WEB SCRAPING OF JOB IN RWANDA**  |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||"
      ],
      "metadata": {
        "id": "g6zfqvFuJC4e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Pmusf2d79_j"
      },
      "outputs": [],
      "source": [
        "class dataMining:\n",
        "  def __init__(self, jobSource = \"Job In Rwanda\"):\n",
        "    \"\"\"\n",
        "    this class has a mission to webscrap jobinrwanda.com and finally return the full text of each job.\n",
        "    \"\"\"\n",
        "    self.source = jobSource\n",
        "    self.jobIdentification = {}\n",
        "  \n",
        "  # CREATE SOUP\n",
        "  def Soup(self, url):\n",
        "    \"\"\"BeautifulSoup object will be created in  this function and returned\"\"\"\n",
        "    page = requests.get(url).content\n",
        "    soup = bs(page, \"html.parser\")\n",
        "    return soup\n",
        "\n",
        "  # JOBS LINKS \n",
        "  def get_links(self):\n",
        "    \"\"\"\n",
        "    output link of the jobs linke https://www.jobinrwanda/software-developer\n",
        "    \"\"\"\n",
        "    soup = self.Soup(url = \"https://www.jobinrwanda.com/jobs/all\")\n",
        "    company_infoLink = []\n",
        "    all_jobsLinks = [\"https://www.jobinrwanda.com\"+link['href'] if \"job\" in link['href'] \n",
        "                     else company_infoLink.append(link.text) \n",
        "                     for div in soup.find_all(\"div\", class_= \"card-body p-2\") \n",
        "                     for link in div.find_all(\"a\")\n",
        "                     ]\n",
        "    all_jobsLinks = [link for link in all_jobsLinks if link]\n",
        "    return (all_jobsLinks, company_infoLink)\n",
        "\n",
        "  # JOBS INFO SUCH AS DEADLINE, JOB TITLE, etc.\n",
        "  def get_jobInfo(self):\n",
        "    \"\"\"\n",
        "    this function will return a dictionary containing information about the job ex: {'deadline':'2022/10/05', 'sector':'Computer and IT', .....}\n",
        "    \"\"\"\n",
        "    jobLinks = self.get_links()[0]\n",
        "    info = {}\n",
        "    \n",
        "    def refine_info(ls):\n",
        "      refined_info = {}\n",
        "      infoZeroPattern = re.compile(\"\\d+.*\")\n",
        "      views = infoZeroPattern.search(ls[0]).group()\n",
        "      refined_info[\"views\"] = views\n",
        "      otherInfoPattern = re.compile(\":\")\n",
        "      \n",
        "      for detail in ls[1:]:\n",
        "        if detail.lower() == \"apply\":\n",
        "          pass\n",
        "        else:\n",
        "          try:\n",
        "            detailed = otherInfoPattern.split(detail)\n",
        "            refined_info[detailed[0]] = detailed[1]\n",
        "          except IndexError:\n",
        "            pass\n",
        "      return refined_info\n",
        "       \n",
        "    for index,url in enumerate(jobLinks):\n",
        "      soup = self.Soup(url)\n",
        "      pattern = re.compile(\"\\s{2,}\")\n",
        "      job_info = [pattern.sub(\" \",li.text.strip().replace(\"\\n\", \"\")) for ul in soup.find_all('ul', class_ = \"list-group list-group-flush\") \n",
        "      for li in ul.find_all('li')]\n",
        "      # DICTIONANRY OF INFO\n",
        "      info[index] = refine_info(job_info[:9])\n",
        "    return info\n",
        "\n",
        "  def get_description(self):\n",
        "    \"\"\"\n",
        "    full text of the job description will be retrived in this function\n",
        "    \"\"\"\n",
        "    jobLinks = self.get_links()[0]\n",
        "    all_text= {}\n",
        "    for index,link in enumerate(jobLinks):\n",
        "      soup = self.Soup(link)\n",
        "      # JOB TITLE\n",
        "      job_title = soup.find('span', class_ = \"field field--name-title field--type-string field--label-hidden\")\n",
        "      \n",
        "      tags_content = soup.find_all('div', class_= \"clearfix text-formatted field field--name-field-job-full-description field--type-text-long field--label-hidden field__item\")\n",
        "      \n",
        "      for div in tags_content:\n",
        "        temp = []\n",
        "        try:\n",
        "          # GET APPLICATION LINK\n",
        "          app_link = soup.find(\"a\", class_ = \"btn btn-sm btn-success\")['href']\n",
        "          if app_link.startswith(\"/\"):\n",
        "            app_link = \"https://www.jobinrwanda.com\"+app_link\n",
        "          temp.append(app_link)\n",
        "          \n",
        "        except TypeError:\n",
        "          temp.append(np.nan)\n",
        "        for tag in div.children:\n",
        "          try:\n",
        "            temp.append(tag.get_text())\n",
        "          except (AttributeError, TypeError):\n",
        "            pass\n",
        "        temp.insert(0, job_title.text)\n",
        "        all_text[index] = temp\n",
        "    return all_text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MERGE PARAGRAPHS OF JOB DESCRPTION TO FORM A VALID TEXT"
      ],
      "metadata": {
        "id": "QTfMb5c-NBVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def final_textDemo():\n",
        "  \"\"\"As the name implies, it is about creating the final text that is ready to be transformed into the dataframe\"\"\"\n",
        "  global info\n",
        "  # CLASS OBJECT CREATION\n",
        "  jobInRwanda = dataMining()\n",
        "  job_text = jobInRwanda.get_description()\n",
        "  info = jobInRwanda.get_jobInfo()\n",
        "\n",
        "  final_demo = []\n",
        "  for index in list(info.keys()):\n",
        "    info_value = list(info[index].values())\n",
        "    # ADD COMPANY HIRING\n",
        "    co_info = jobInRwanda.get_links()[1]\n",
        "    info_value.append(co_info[index])\n",
        "    # INCLUDE JOB TITLE\n",
        "    info_value.append(job_text[index][0])\n",
        "    # INCLUDE APPLY LINK\n",
        "    info_value.append(job_text[index][1])\n",
        "    # JOIN PARAGRAPHS \n",
        "    t = '\\n'.join(job_text[index][2:])\n",
        "    g = re.compile(\"\\n|\\\\xa0\").sub(\"\", t)\n",
        "    info_value.append(g)\n",
        "    final_demo.append(info_value)\n",
        "  return final_demo"
      ],
      "metadata": {
        "id": "tCezIV89NMX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CREATE THE DATAFRAME OF THE OUTPUT JOB DETAILS"
      ],
      "metadata": {
        "id": "RzeO_D2vNzug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def makeDataFrame(dataList, columns):\n",
        "  return pd.DataFrame(dataList, columns = columns)"
      ],
      "metadata": {
        "id": "YrLYv_KzN6yZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FILTER THE DATAFRAME TO REMAIN ONLY WITH IT & COMPUTER RELATED JOBS"
      ],
      "metadata": {
        "id": "jVwl373XOBRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Is IT Job'] = False\n",
        "def isIT_Job():\n",
        "  for index in range(len(df)):\n",
        "      if \"Computer\" in df.iloc[index]['Sector'] or \"IT\" in df.iloc[index]['Sector']:\n",
        "        df['Is IT Job'][index] = True"
      ],
      "metadata": {
        "id": "I9IwLt34N_-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ONLY IT & COMPUTER RELATED JOBS"
      ],
      "metadata": {
        "id": "sRBTk-4ZPIBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "computer_and_IT_jobs = df[df['Is IT Job'] == True]"
      ],
      "metadata": {
        "id": "S4Qnagv1PL9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||  **WEB  SCRAPING OF UMUCYO WEBSITE**  |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||"
      ],
      "metadata": {
        "id": "-dOZElnmRkT1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GUxN8z4CRuHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||  **JOB DESCRIPTION TRANSLATION ZONE**  |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||"
      ],
      "metadata": {
        "id": "Od6YRS-cQtO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class translator:\n",
        "  def __init__(self):\n",
        "    # INITIALIZE THE MODEL FOR TRANSLATE\n",
        "    self.model = EasyNMT(\"opus-mt\")\n",
        "  \n",
        "  def supportedLangs(self, all_langs, supported):\n",
        "    with open(all_langs) as f_json:\n",
        "      content = f_json.read()\n",
        "    langs = json.loads(content)\n",
        "\n",
        "    with open(supported) as f2_json:\n",
        "          content = f2_json.read()\n",
        "    supplangs = json.loads(content)\n",
        "    supp_codes = set(list(langs.values())).intersection(set(supplangs))\n",
        "    supp_keyValue = {}\n",
        "    for key in langs.keys():\n",
        "      if langs[key] in supp_codes:\n",
        "        supp_keyValue[key] = langs[key]\n",
        "    return supp_keyValue\n",
        "    \n",
        "\n",
        "  def translate(self, sentences, targetLang, source = 'en', single = True):\n",
        "    fail_message = \"[FAIL]: destination lang is not a suported language by the model\"\n",
        "    success = 0\n",
        "    if single:\n",
        "      try:\n",
        "        translation = self.model.translate(sentences, source_lang = source, target_lang = targetLang)\n",
        "        return translation\n",
        "      except:\n",
        "        print(fail_message)\n",
        "    else:\n",
        "      translated = []\n",
        "      for target in targetLang:\n",
        "        try:\n",
        "          translation = self.model.translate(sentences, source_lang = source, target_lang = target)\n",
        "          translated.append(translation)\n",
        "          success += 1\n",
        "        except:\n",
        "          print(fail_message)\n",
        "      return translated, success\n",
        "\n",
        "# selenium : https://www.browserstack.com/guide/web-scraping-using-selenium-python#:~:text=Selenium%20is%20needed%20in%20order,of%20the%20browser%20being%20used."
      ],
      "metadata": {
        "id": "VMshDBBPQ5xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||  **API CREATION ZONE**  |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||"
      ],
      "metadata": {
        "id": "QIaLAFfKRwpx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z3Jd7n5WRv5E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}